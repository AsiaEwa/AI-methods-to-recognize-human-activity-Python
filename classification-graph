from torch.utils.data import Dataset, DataLoader
from torch import nn, from_numpy, optim
from torch import cuda
import numpy as np
import torch
import time
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from torch.autograd import Variable
from sklearn.metrics import confusion_matrix


print(  torch.__version__)#device = 'cuda' if cuda.is_available() else 'cpu' na CPU; wybrana opcja pracy na: GPU 'cuda'
device='cuda' #device='cpu' 
Epoch_num = 95 #100 80 
batch_size = 24  #24  #16 #32 - sprawdzanie dziania na roznych wartociach
train_loss_values = np.zeros([Epoch_num - 1])
test_loss_values = np.zeros([Epoch_num - 1])
scaler = StandardScaler()

class Train_ActivitiesDataset(Dataset):
    """ Activities dataset."""
    # Initialize data, download, etc.
    def __init__(self):
       
        xy = np.loadtxt('/home/asia/Desktop/opracowanie wynikow/gotowe/zrobione7csv/treningowy-80.csv',delimiter=',', dtype=np.float32)
        self.len = xy.shape[0]

        self.x_data = from_numpy(xy[:, :-1])
        # scaler.fit(self.x_data)
        # self.x_data  = scaler.transform(self.x_data)
        self.y_data = from_numpy(xy[:, [-1]])

    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    def __len__(self):
        return self.len

class Test_ActivitiesDataset(Dataset):
    """ Activties dataset."""
    # Initialize your data, download
    def __init__(self):
        
        xy = np.loadtxt('/home/asia/Desktop/opracowanie wynikow/gotowe/zrobione7csv/testowy-20.csv',delimiter=',', dtype=np.float32)

        self.len = xy.shape[0]
        self.x_data = from_numpy(xy[:, :-1])
        # scaler.fit(self.x_data)
        # self.x_data = scaler.transform(self.x_data)
        self.y_data = from_numpy(xy[:, [-1]])

    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    def __len__(self):
        return self.len

#all_dataset = Train_ActivitiesDataset()
train_dataset = Train_ActivitiesDataset()
test_dataset = Test_ActivitiesDataset()
#train dataset=trains dstest_dataset = Test_ActivitiesDataset()

train_loader = DataLoader(dataset=train_dataset,
                          batch_size=batch_size,
                          shuffle=True,
                          num_workers=0)
test_loader = DataLoader(dataset=test_dataset,
                         batch_size=batch_size,
                         shuffle=True)

# To print layer outputs
class PrintLayer(nn.Module):
    def __init__(self):
        super(PrintLayer, self).__init__()

    def forward(self, x):
        # Do your print / debug stuff here

        # 1. Printing layer output as Tensor form
        print(x)
        # 2. Printing layer output as Numpy form
        print(x.detach().numpy())
        return x

# Method 2 for - Checking Layer outputs
class Model(nn.Module):

    def __init__(self):
        
        super(Model, self).__init__()
        # self.l1 = nn.Linear(10, 16) Wejscie klatek, liczba neuronow 
        # self.l1 = nn.Linear(10, 16) Wej
        self.l1 = nn.Linear(480, 90) #480, 90
        self.l2 = nn.Linear(90, 90)  #90,16
        self.l3 = nn.Linear(90, 7)    #16, 7- liczba klas =liczba aktywnosci

# funkcje aktywacji:
        self.sigmoid = nn.Sigmoid()
        self.softmax = nn.Softmax()
        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()
    def forward(self, x):
        
        x = self.relu(self.l1(x))
        x = self.relu(self.l2(x))
        return self.l3(x)
# our model
model = Model()
model.to(device)

## Method 1 for - Checking Layer outputs
# model = nn.Sequential(
#         nn.Linear(10, 16),
#         nn.ReLU(),
#         #PrintLayer(),
#         nn.Linear(16, 16),
#         nn.ReLU(),
#         nn.Linear(16, 4),
#         )
# model.to(device)
# optymalizator Adam:
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.002)

def train(epoch, train_loss_values): #funkcja straty
    train_loss = 0

    for batch_idx, (inputs, labels) in enumerate(train_loader):
        # get the inputs
        inputs, labels = inputs.to(device=device).float(), labels.to(device=device).float()

        #e Forward pass: Compute predicted y by passing x to the model
        optimizer.zero_grad()
        y_pred = model(inputs)
        labels = torch.reshape(labels, [-1]).to(device)

        loss = criterion(y_pred, labels.long())
        train_loss +=loss
        loss.backward()

        optimizer.step()

        if batch_idx % 10 == 0:
            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(
                epoch, batch_idx * len(inputs), len(train_loader.dataset),
                       100. * batch_idx / len(train_loader), loss.item()))
    print(train_loss)
    train_loss /= len(train_loader.dataset)
    #train_loss_values.append(loss / len(inputs))
    #train_loss_values = np.append(train_loss_values, train_loss)
    train_loss_values[epoch-1] = train_loss

    
    torch.save(model.state_dict(), '/home/asia/Desktop/opracowanie wynikow/przyklad-net-dane-diabetykow/PyTorch-1D-NN-Classifier-master/simple_ann_classifier_w_test/trained_model/trained_model.pt')

    return train_loss_values

def test(test_loss_values):
    #model.eval()
    test_loss = 0
    correct = 0
  
    all_pred=[]
    all_labels=[]

    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device=device).float(), labels.to(device=device).float()
        y_pred = model(inputs) #to wejscie - wykres
        print('wejscie testowe sieci')
        #print(inputs.size())
        labels = torch.reshape(labels, [-1]).to(device)
        print(labels.size())
        
        # sum up batch loss
        loss = criterion(y_pred, labels.long()).item()
        test_loss +=loss

        # get the index of the max
        pred = y_pred.data.max(1, keepdim=True)[1]
        correct += pred.eq(labels.data.view_as(pred)).cpu().sum()
        print(pred)    

        #confusion_matrix(labels.tolist(), pred.tolist())
        #print(confusion_matrix(labels.tolist(), pred.tolist()))
        
        all_pred+=pred.tolist()
        all_labels+=labels.tolist()

   
    confusion_matrix(all_labels, all_pred)
    print(confusion_matrix(all_labels, all_pred))

    test_loss /= len(test_loader.dataset)
    #test_loss_values.append(test_loss)
    # = np.append(test_loss_values, test_loss)
    test_loss_values[epoch-1] = test_loss

    print(f'===========================\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '
          f'({100. * correct / len(test_loader.dataset):.0f}%)')
    return test_loss_values # to wyjscie - wykres
    print(y_pred)
    #print(pred)

def test2():
    #model.eval()
    test_loss = 0
    correct = 0
  
    all_pred=[]
    all_labels=[]

    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device=device).float(), labels.to(device=device).float()
        y_pred = model(inputs) #to wejscie - wykres
        print('wejscie testowe sieci')
        #print(inputs.size())
        labels = torch.reshape(labels, [-1]).to(device)
        print(labels.size())
        
        # sum up batch loss
        loss = criterion(y_pred, labels.long()).item()
        test_loss +=loss

        # get the index of the max
        pred = y_pred.data.max(1, keepdim=True)[1]
        correct += pred.eq(labels.data.view_as(pred)).cpu().sum()
        print(pred)    #zakomentowane

        #confusion_matrix(labels.tolist(), pred.tolist())
        #print(confusion_matrix(labels.tolist(), pred.tolist()))
        
        all_pred+=pred.tolist()
        all_labels+=labels.tolist()

    conf_mx=confusion_matrix(all_labels, all_pred)
    print(conf_mx)
    plt.matshow(conf_mx, cmap=plt.cm.gray)
    plt.show()

    return  

if __name__ == '__main__':
    since = time.time()
    test_loss_values[0]
    for epoch in range(1, Epoch_num):
        epoch_start = time.time()
        train_loss_values = train(epoch, train_loss_values)
        m, s = divmod(time.time() - epoch_start, 60)
        print(f'Training time: {m:.0f}m {s:.0f}s')
        test_loss_values = (test(test_loss_values)) #dodane[epoch]
        m, s = divmod(time.time() - epoch_start, 60)
        print(f'Testing time: {m:.0f}m {s:.0f}s')

    m, s = divmod(time.time() - since, 60)
    print(f'Total Time: {m:.0f}m {s:.0f}s\nModel was trained on {device}!')
    #print(train_loss_values)
    #print(test_loss_values)

    x_len = np.arange(len(train_loss_values))

    test_loss_values_mean = np.mean(test_loss_values, axis=0)
    test_standard_dev = np.std(test_loss_values, axis=0)
    train_loss_values_mean = np.mean(train_loss_values, axis=0)
    train_standard_dev = np.std(train_loss_values, axis=0)

    test2()
   
    plt.figure(2)
    plt.plot( x_len ,test_loss_values, marker = '.',label="funkcja straty zbioru testowego")
    #plt.plot( x_len ,train_loss_values,marker = '.', label="funkcja straty zbioru treningowego")
    plt.legend(loc='upper right')
    plt.grid()
    plt.xlabel('epoka')
    plt.ylabel('strata')
    plt.show()
